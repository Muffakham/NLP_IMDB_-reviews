{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6vjrQK8i-6Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "The Dataset IMDB Movie Reviews dataset\n",
        "---\n",
        " \n",
        "To find if a review is positive or not.\n",
        "Loading the dataset here. The IMDB dataset is built into keras and has been pre-processed. \n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "*   The data is basicaly reviews of the movie\n",
        "*   All the words in the review have been replaced by the frequency of occurence of each word. example: \"This is a good movie\" now if we consider that each word is replaced by its frequency. let \"This\" have a frequency of 1000, is = 200, good = 3000 movie = 10000. Now the review becomes\n",
        "1000 200 3000 100000\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCHdL-LuYYig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSgjkpFRllLq",
        "colab_type": "text"
      },
      "source": [
        "In this cell we are loading the dataset and only selecting the reviews which have words that max out at a frequency of 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UarTuSE6Yk7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "e3673c9f-20a3-407a-9c15-afb2db1c3695"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "print(X_train[0])\n",
        "print(y_train[0])\n",
        "print('Shape of training data: ')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print('Shape of test data: ')\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "1\n",
            "Shape of training data: \n",
            "(25000,)\n",
            "(25000,)\n",
            "Shape of test data: \n",
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co2N4XQhljvy",
        "colab_type": "text"
      },
      "source": [
        "In this cell we are selecting the reviews which have a maximum of 450 words. In case the review is not 450 words long, then it is padded with extra values, in this case (zero's) to make it 450 words long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZadcP8fJY2bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words = 450\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm6dAiBpmDe5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "The Model: Neural Network and its creation\n",
        "---\n",
        "\n",
        "\n",
        "*   In this cell we creae the model that trains upon the datset to give us the desired results\n",
        "*   We are using a Convolution nueral network with 6 different layers.\n",
        "\n",
        "\n",
        "> Indented block\n",
        "* The layer 1: Embedding layer that converts the input which are reviews whose input dimension is 10000 and we are reducing it to 32 dimensions.  [For more info on embedding layer](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work)\n",
        "* layer 2: Convoution layer, a 1 domesnsional convolution layer that passes over the input (32 dimesnion vector) and performs the 'relu' function on it.\n",
        "* layer 3: Max pool laying layer that reduces the lenth of the array by cosidering only the maximum value from a 2x2 array that passes over the entire input\n",
        "* layer 4: flatten layer that converts the mxn dimensional array into 1x (m*n) array\n",
        "* layer 5: It is a dense layer that reduces the size of the input by performing relu on it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCjWbqXIZEQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(10000, 32, input_length=max_words))\n",
        "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyP3bpR7ZRMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "42b923af-618b-477a-a47e-064cc147ad5c"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 450, 32)           320000    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 450, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 225, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 7200)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 250)               1800250   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 2,123,605\n",
            "Trainable params: 2,123,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp0v2x2CiTg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "c6374ffd-65fd-407c-d71e-2134435aab98"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 - 22s - loss: 0.4134 - accuracy: 0.7880 - val_loss: 0.2690 - val_accuracy: 0.8889\n",
            "Epoch 2/2\n",
            "196/196 - 22s - loss: 0.1785 - accuracy: 0.9338 - val_loss: 0.2821 - val_accuracy: 0.8822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f422115afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5NT2ArpidES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50639e4a-12f0-40d6-e8c9-cfc4bb2b45b5"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 88.22%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}